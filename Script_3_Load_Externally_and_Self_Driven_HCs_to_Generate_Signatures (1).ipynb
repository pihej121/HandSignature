{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check python version \n",
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tensorflow version\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.model_selection as model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import keras as k\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import scipy.io\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import interpolate\n",
    "from scipy.special import iv\n",
    "from numpy import sin,cos,pi,array,linspace,cumsum,asarray,dot,ones\n",
    "from pylab import plot, legend, axis, show, randint, randn, std,lstsq\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/victorphilippecodex/Documents/Hand_Signature/WholeGroup1/Python_Inputs\n",
    "\n",
    "# Ensure fourierseries.py is in the pathway\n",
    "!ls -l fourierseries.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import phaser\n",
    "import dataloader\n",
    "# Preprocess data for a single subject - to be send to modeling frameworks\n",
    "def find_phase(k):\n",
    "    \"\"\"\n",
    "    Detrend and compute the phase estimate using Phaser\n",
    "    INPUT:\n",
    "      k -- dataframe\n",
    "    OUTPUT:\n",
    "      k -- dataframe\n",
    "    \"\"\"\n",
    "    #l = ['hip_flexion_l','hip_flexion_r'] # Phase variables = hip flexion angles\n",
    "    y = np.array(k)\n",
    "    print(y.shape)\n",
    "    y = util.detrend(y.T).T\n",
    "    print(y.shape)\n",
    "    phsr = phaser.Phaser(y=y)\n",
    "    k[:] = phsr.phaserEval(y)[0,:]\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vonMies(t,t_0, b):\n",
    "    out = np.exp(b*np.cos(t-t_0))/(2*pi*iv(0, b))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to save the models and read data from\n",
    "path = '/home/victorphilippecodex/Documents/Hand_Signature/WholeGroup1'\n",
    "\n",
    "# Insert the directory\n",
    "import sys\n",
    "sys.path.insert(0,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-changing variables \n",
    "\n",
    "# number of trials in dataset \n",
    "trialnum = 252 # 72 total trials\n",
    "\n",
    "# number of samples in each trial\n",
    "trialsamp = 2100\n",
    "\n",
    "# number of features collected per trial\n",
    "feats = 15\n",
    "\n",
    "#Batch size - same as the number of traintrials\n",
    "batch_size = trialnum\n",
    "\n",
    "# Number of Layers\n",
    "numlayers = 1\n",
    "\n",
    "# Choose the number of iterations to train the model- if this script has been run previously enter a value greater than was \n",
    "# inputted before and rerun the script. \n",
    "finalepoch = 10000\n",
    "\n",
    "# load the input data/kinematics\n",
    "datafilepath = '/home/victorphilippecodex/Documents/Hand_Signature/WholeGroup1/DownsampledGroup1.csv' #input data\n",
    "all_csvnp = np.loadtxt(datafilepath,delimiter=',').T\n",
    "\n",
    "# reshape all the input data into a tensor\n",
    "all_inputdata_s = all_csvnp.reshape(trialnum,trialsamp,feats) \n",
    "csvnp = all_inputdata_s\n",
    "print('original input data shape is: '+ str(all_csvnp.shape ))\n",
    "print('input data reshaped is: '+ str(all_inputdata_s.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of models and corresponding parameters to test \n",
    "test_model_nodes = [256] \n",
    "seqs = [699] #lookback parameter\n",
    "\n",
    "# run multiple model architechtures many times to test stability of cost function outputs\n",
    "runs = 1 # stability analysis - repeat each model architecture this many times\n",
    "test_model_seq = np.repeat(seqs, runs)\n",
    "\n",
    "count = np.arange(runs)\n",
    "\n",
    "All_nodes = np.empty([0,1], dtype='int')\n",
    "All_seq = np.empty([0,1],dtype='int')\n",
    "All_valseg = np.empty([0,1],dtype='int')\n",
    "All_trainseg = np.empty([0,1],dtype='int')\n",
    "All_modelname = []\n",
    "All_mod_name = []\n",
    "count = np.empty([0,1],dtype='int'); #initialize model run -- this serves as the model run ID number\n",
    "ct = 0\n",
    "for a in test_model_nodes:\n",
    "  for b in test_model_seq:\n",
    "    count = np.append(count,  ct + 1 )\n",
    "    #if statement for valseg, trainseg based on sequence length\n",
    "    if int(b) == 699:\n",
    "      trainseg = 2\n",
    "      valseg = 1\n",
    "    \n",
    "\n",
    "    All_nodes = np.append(All_nodes, a) \n",
    "    All_seq = np.append(All_seq, int(b))\n",
    "    All_valseg = np.append(All_valseg, valseg)\n",
    "    All_trainseg = np.append(All_trainseg, trainseg)\n",
    "    All_modelname = np.append(All_modelname, 'UNIT_' + str(a) + '_LB_' + str(b) + '_run_' + str(count[-1]) + '/' )\n",
    "    All_mod_name = np.append(All_mod_name, 'UNIT_' + str(a) + '_LB_' + str(b) + '_run_' + str(count[-1]) )\n",
    "\n",
    "    if ct+1 < runs:\n",
    "      ct += 1\n",
    "    else:\n",
    "      ct = 0\n",
    "\n",
    "print(All_mod_name)\n",
    "j = 0\n",
    "print(path + All_modelname[j] + All_mod_name[j] + '_bestwhole.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(All_mod_name)):\n",
    "    # make folder to store model\n",
    "    newfoldpath = path + All_mod_name[j]\n",
    "\n",
    "\n",
    "    # extract path to store each model and generated data\n",
    "    savepath = path + All_modelname[j]\n",
    "    mod_name = All_mod_name[j]\n",
    "\n",
    "    print('Working on: ' + mod_name + ' model ' + str(j) + ' / ' + str(len(All_mod_name)))\n",
    "    \n",
    "    # Specify variables for model run instance\n",
    "\n",
    "    # Number of Units\n",
    "    numunits = All_nodes[j]\n",
    "    \n",
    "    test_ind = np.arange(0, len(all_inputdata_s), 1)  # run all input data through\n",
    "\n",
    "    ext_drive= trialsamp\n",
    "\n",
    "    tot_len = len(test_ind)\n",
    "\n",
    "    # PCA of HCs\n",
    "\n",
    "    ExternalDriveHCs_testset = np.load(savepath + mod_name + '_extdriveHCs_testset.npy')\n",
    "    SelfDriveHCs_testset = np.load(savepath + mod_name + '_selfdriveHCs.npy')\n",
    "\n",
    "    X = ExternalDriveHCs_testset #PCA of only externally driven\n",
    "\n",
    "    pca = PCA(n_components=numunits*2) #initialize\n",
    "\n",
    "    X_reduction = pca.fit_transform(X) # X is projected on the first principal components previously extracted from a training set - data is centered here\n",
    "\n",
    "    np.save(savepath + mod_name + '_pca_var.npy', pca.explained_variance_ratio_[0:6])\n",
    "\n",
    "    tot_len = len(test_ind) # len of all trials\n",
    "    HC_CellArray = np.empty(shape=[tot_len, ext_drive-100, X_reduction.shape[1]])\n",
    "    start = 0\n",
    "    last = ext_drive # length of trials\n",
    "    for p in range(tot_len):\n",
    "        HC_CellArray[p]= X_reduction[start+100:last]\n",
    "        start = start+ext_drive\n",
    "        last = last+ext_drive\n",
    "\n",
    "    scipy.io.savemat(savepath + mod_name + '_HCvalues.mat', {'HC_CellArray': HC_CellArray})\n",
    "    trialsamp = trialsamp- 100\n",
    "    # Generate phase averaged signatures\n",
    "    print('generating phase averaged signatures')\n",
    "    # phase length = 100\n",
    "    lim = tot_len\n",
    "    L = HC_CellArray.shape[2] # lets look at 1st 6 to speed things up #numunits*2  # number of PCs or units to be phase averaged\n",
    "    PhaseAveragedPCs = np.empty(shape=[lim, L, 100])\n",
    "    PhaseAveragedPCs_shift = np.empty(shape=[lim, L, 100])\n",
    "    Phase_Variables = np.empty(shape=[lim, trialsamp])\n",
    "    cyclephase = []\n",
    "    PC_Shifts = np.empty(shape=[lim]) # store phase shift values\n",
    "    # Average orbits initialization\n",
    "    numSegments = 100 # we want each Phase averaged orbit to be 100 sample points long\n",
    "    phaseVals = np.linspace(0, 2*pi, numSegments, endpoint=True) # phases that we want our phase averaged orbits to correspond to\n",
    "    kappa = 20 \n",
    "\n",
    "    for a in range(lim): #for length of all the trials \n",
    "        print(['processing trial: ' + str(a)]) \n",
    "        dats=[] # reset variables after each trial - kinematics\n",
    "        dats2 = [] # reset the HC params\n",
    "        allsigs = np.empty(shape=[100,])\n",
    "        all_phase_var = np.empty(shape=[100,]) \n",
    "        PhaseAvgPCs = np.empty(shape=[100])\n",
    "        all_cyclephase = []\n",
    "        raw = HC_CellArray[a][:,(0,0)].T # Only use 1st 3 PCs (of HCs),\n",
    "        #raw = HC_CellArray[a][:,0:2].T\n",
    "        #raw = raw[:,np.newaxis]\n",
    "        raw2 = HC_CellArray[a][:,0:L].T  # extract all the HC data\n",
    "        for b in range(6): #Shai's code does duplication- works better than without duplicating data\n",
    "            dats.append(raw-raw.mean(axis=1)[:,np.newaxis]) #center the HCs data for phase estimation -- this centers each one separately\n",
    "            dats2.append(raw2-raw2.mean(axis=1)[:,np.newaxis])\n",
    "        #dats = np.swapaxes(dats,1,2)\n",
    "        rHC = dats2[0] #centered\n",
    "        #rHC = raw2 #uncentered \n",
    "        phr = phaser.Phaser(dats) # use centered data from 1st 3 PCs\n",
    "        phi = [ phr.phaserEval( d ) for d in dats ] # extract phase\n",
    "        phi2  =(phi[0].T % (2*pi)); # Take modulo s.t. phase ranges from [0,2*pi]\n",
    "         \n",
    "        # find average orbits\n",
    "        avgOrbits = np.zeros((numSegments,L)) #initialize avg orbits\n",
    "        phases = np.reshape(phi2,[trialsamp,]) #this was the error!!!!!\n",
    "\n",
    "        for c in range(numSegments): #number of points in final average orbit/phase averaging\n",
    "            vonMiesCurrent = vonMies(phases,phaseVals[c],kappa) # for each value in the num of phase points calculate current\n",
    "            sumVal = np.sum(vonMiesCurrent)\n",
    "\n",
    "            for d in range(L):  # for the number of features or units\n",
    "                data = np.reshape(rHC[d,:],[trialsamp,1])\n",
    "                avgOrbits[c,d] = np.sum(data.T*vonMiesCurrent)/sumVal # phase point (row), feature of PC (column) -- this is generating a value for the 1st phase point of each feature\n",
    "\n",
    "        PhaseAveragedPCs[a] = avgOrbits.T # transform to match overall saving structure as before\n",
    "        phi2 = phi2.reshape(trialsamp,)\n",
    "        Phase_Variables[a] = phi2 # store phase variables for all cycles/features - may need to phase shift these \n",
    "        \n",
    "        # Phase shift the data according to PC1 max align with zero phase\n",
    "        PC1_maxloc = np.argmax(PhaseAveragedPCs[a][0]) # only PC1 value (1st max if repeated)\n",
    "        Data2Shift = PhaseAveragedPCs[a]\n",
    "        NewP = np.roll(Data2Shift, -PC1_maxloc,axis=1) #shift back to orgin\n",
    "        PC_Shifts[a] = PC1_maxloc\n",
    "        PhaseAveragedPCs_shift[a] = NewP\n",
    "\n",
    "    np.save(savepath + mod_name + '_PhaseAvgPcs_shift.npy', PhaseAveragedPCs_shift)\n",
    "    scipy.io.savemat(savepath + mod_name + '_PhaseAvgPcs_shift.mat', {'PhaseAvgPCs_shift': PhaseAveragedPCs_shift})\n",
    "\n",
    "    np.save(savepath + mod_name + '_PhaseAvgPcs.npy', PhaseAveragedPCs)\n",
    "    scipy.io.savemat(savepath + mod_name + '_PhaseAvgPcs.mat', {'PhaseAvgPCs': PhaseAveragedPCs})\n",
    "\n",
    "    np.save(savepath + mod_name + '_PhaseVariables.npy', Phase_Variables)\n",
    "    scipy.io.savemat(savepath + mod_name + '_PhaseVariables.mat', {'PhaseVariables': Phase_Variables})\n",
    "\n",
    "\n",
    "    # Concatenate gait signature per trial\n",
    "    # Make a single phase averaged signature per row\n",
    "    PA_shape = PhaseAveragedPCs_shift.shape\n",
    "    gait_sig_size = PA_shape[1]*PA_shape[2]\n",
    "    Gaitsignatures = np.empty(shape = [len(PhaseAveragedPCs_shift),gait_sig_size]) #change to shifted PCS! \n",
    "    Gaitsignatures_trunc6 = np.empty(shape = [len(PhaseAveragedPCs_shift),600])\n",
    "    for h in range(len(PhaseAveragedPCs_shift)):\n",
    "        reshape_sig = PhaseAveragedPCs_shift[h].reshape(1,gait_sig_size)\n",
    "        Gaitsignatures[h] = reshape_sig[0]\n",
    "        Gaitsignatures_trunc6[h] = reshape_sig[0][0:600] #truncate to 1st 6 PCs\n",
    "    np.save(savepath + mod_name + '_Gaitsignatures.npy', Gaitsignatures)\n",
    "    scipy.io.savemat(savepath + mod_name + '_Gaitsignatures.mat', {'GaitSigs': Gaitsignatures})\n",
    "\n",
    "    np.save(savepath + mod_name + '_Gaitsignatures_trunc6.npy', Gaitsignatures_trunc6)\n",
    "    scipy.io.savemat(savepath + mod_name + '_Gaitsignatures_trunc6.mat', {'GaitSigs': Gaitsignatures_trunc6})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Gaitsignatures_trunc6[0].T)\n",
    "plt.plot(Gaitsignatures_trunc6[27].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#datafilepath3 = '/home/victorphilippecodex/Downloads/SpeedLabelsPatrick.mat'\n",
    "#speedlabels = scipy.io.loadmat(datafilepath3)\n",
    "\n",
    "#speedlist = []\n",
    "#for m in test_ind:\n",
    "    #speedlist.append(speedlabels['FingTrials'][m][0][0][0])\n",
    "\n",
    "#myfile = '/home/victorphilippecodex/Downloads/Subjectlabels .csv'\n",
    "#csvfile = open(myfile, 'r')\n",
    "#reader = csv.reader(csvfile, delimiter='\\t')\n",
    "#allsubs = list(reader)\n",
    "#select_subs = map(allsubs.__getitem__, test_ind)\n",
    "#subs= list(select_subs)\n",
    "\n",
    "\n",
    "# Mutlidimensional scaling of gait signatures\n",
    "embedding = MDS(n_components=3)\n",
    "X_transformed = embedding.fit_transform(Gaitsignatures_trunc6)\n",
    "np.save(savepath + mod_name + '_MDS_X_transformed.npy', X_transformed)\n",
    "scipy.io.savemat(savepath + mod_name + '_MDS_X_transformed.mat', {'X_transformed': X_transformed})\n",
    "\n",
    "\n",
    "# MDS plots of ext and self sigs\n",
    "#fig = plt.figure()\n",
    "#plt.scatter(X_transformed[:,0],X_transformed[:,1],c = speedlist)\n",
    "# Loop for annotation of all points\n",
    "#for i in range(len(subs)):\n",
    "        \n",
    "    #plt.annotate(str(count), (X_transformed[i,0], X_transformed[i,1] + 0.3))\n",
    "    #count = count + 1    \n",
    "#plt.savefig(savepath + mod_name +  '_MDS.png', dpi = 300)\n",
    "#plt.close(fig)# close figure in loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(HC_CellArray[3,:,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(HC_CellArray[91,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(HC_CellArray[14,:,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(\"/home/victorphilippecodex/Documents/Hand_Signature/Individual0011UNIT_256_LB_4999_run_1/UNIT_256_LB_4999_run_1_pca_var.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e5ecc7f1ed5ad2012e040649f3b38678c25a0603972bc6e06040cc507d21099"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
